{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression"
      ],
      "metadata": {
        "id": "Q6vLSx4Fv_Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " from sklearn import datasets, linear_model, metrics\n",
        " from sklearn.datasets import fetch_california_housing\n",
        "\n",
        " california_housing = fetch_california_housing()\n",
        " X=california_housing.data # Features (X)\n",
        " y =california_housing.target # Target (y)\n",
        " from sklearn.model_selection import train_test_split\n",
        "\n",
        " X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=1)\n",
        "\n",
        " reg=linear_model.LinearRegression()\n",
        " reg.fit(X_train,y_train)\n",
        " print('Coefficients:',reg.coef_)\n",
        " print('Variance score: {}'.format(reg.score(X_test,y_test)))\n",
        "\n",
        " plt.scatter(reg.predict(X_test),reg.predict(X_test)-y_test,color=\"blue\",s = 10,label = \"Test\n",
        " data\")\n",
        " plt.scatter(reg.predict(X_train),reg.predict(X_train)-y_train,color=\"green\",s = 10,label =\n",
        " \"Train data\")\n",
        "\n",
        " #plt.hlines(y = 0,xmin = 0,xmax = 50,linewidth = 2)\n",
        " #plt.legend(loc = \"upper right\")\n",
        " plt.xlabel('x-axis', fontsize=20)\n",
        " plt.ylabel('y-axis', fontsize=20)\n",
        "\n",
        " plt.title(\"Residual errors\")\n",
        "\n",
        " plt.grid()\n",
        "\n",
        " plt.show()"
      ],
      "metadata": {
        "id": "OR3vV45swKYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regressions"
      ],
      "metadata": {
        "id": "ZLoTW9ESwprp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules and packages\n",
        " import pandas as pd\n",
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " import seaborn as sns\n",
        " from sklearn.model_selection import train_test_split\n",
        " from sklearn.linear_model import LinearRegression\n",
        " from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        " from sklearn import preprocessing\n",
        " # importing data\n",
        " df = pd.read_csv('/content/Real-estate1.csv')\n",
        " df.drop('No', inplace=True, axis=1)\n",
        " print(df.head())\n",
        " print(df.columns)\n",
        " # plotting a scatterplot\n",
        " sns.scatterplot(x='X4 number of convenience stores',\n",
        " y='Y house price of unit area', data=df)\n",
        " # creating feature variables\n",
        " X = df.drop('Y house price of unit area', axis=1)\n",
        " y = df['Y house price of unit area']\n",
        " print(X)\n",
        " print(y)\n",
        " # creating train and test sets\n",
        " X_train, X_test, y_train, y_test = train_test_split(\n",
        " X, y, test_size=0.2, random_state=101)\n",
        " # creating a regression model\n",
        " model = LinearRegression()\n",
        " # fitting the model\n",
        " model.fit(X_train, y_train)\n",
        " # making predictions\n",
        " predictions = model.predict(X_test)\n",
        " # model evaluation\n",
        " print('mean_squared_error : ', mean_squared_error(y_test, predictions))\n",
        " print('mean_absolute_error : ', mean_absolute_error(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "A5Fdetk8w0Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression algorithm."
      ],
      "metadata": {
        "id": "vWLIayeiw67j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        " from sklearn.model_selection import train_test_split\n",
        " from sklearn import linear_model, metrics\n",
        " from sklearn.metrics import confusion_matrix\n",
        " import seaborn as sns\n",
        " import matplotlib.pyplot as plt\n",
        " # Load your dataset\n",
        " # Replace 'your_dataset.csv' with the path to your CSV\n",
        " file df = pd.read_csv('/content/home (1).csv')\n",
        " # Assume 'target' is the column with labels and the rest are\n",
        " features X = df.drop(columns='Area_Square_Feet')\n",
        " y = df['Area_Square_Feet']\n",
        " # Split X and y into training and testing sets\n",
        " X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        " test_size=0.3, random_state=1)\n",
        " # Create logistic regression object\n",
        " reg = linear_model.LogisticRegression(max_iter=200)\n",
        " # Train the model using the training sets\n",
        " reg.fit(X_train, y_train)\n",
        " # Make predictions on the testing set\n",
        " y_pred = reg.predict(X_test)\n",
        " # Compare actual response values (y_test) with predicted response\n",
        " values (y_pred)\n",
        " accuracy = metrics.accuracy_score(y_test, y_pred) * 100\n",
        " print(\"Logistic Regression model accuracy (in %):\",\n",
        " accuracy) # Print confusion matrix\n",
        " cf_matrix = confusion_matrix(y_test, y_pred)\n",
        " print(\"Confusion Matrix:\\n\", cf_matrix)\n",
        " # Print confusion matrix using Seaborn library\n",
        " sns.heatmap(cf_matrix, annot=True, fmt='d',\n",
        " cmap='Blues') plt.xlabel('Predicted Label')\n",
        " plt.ylabel('True Label')\n",
        " plt.title('Confusion Matrix Heatmap')\n",
        " plt.show()"
      ],
      "metadata": {
        "id": "WSpVCI5LxJNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bagging algorithm."
      ],
      "metadata": {
        "id": "zuF9EtFwxKLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import files\n",
        " upload =files.upload()\n",
        " import pandas as pd\n",
        " df = pd.read_csv('diabetes.csv')\n",
        " df.head()\n",
        "\n",
        "  df.isnull().sum()\n",
        "\n",
        "df.describe()\n",
        "\n",
        " df.Outcome.value_counts()\n",
        "\n",
        "  x = df.drop(\"Outcome\",axis = \"columns\")\n",
        " y = df.Outcome\n",
        " from sklearn.preprocessing import StandardScaler\n",
        " scaler = StandardScaler()\n",
        " x_scaled = scaler.fit_transform(x)\n",
        " x_scaled[:3]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        " x_train,x_test,y_train,y_test = train_test_split(x_scaled,y,stratify = y,random_state = 10)\n",
        " X_train.shape\n",
        " (576, 8)\n",
        " X_test.shape\n",
        " (192, 8)\n",
        " y_train.value_counts()\n",
        "\n",
        "201/375\n",
        "\n",
        "y_test.value_counts()\n",
        "\n",
        "67/125\n",
        "\n",
        " from sklearn.model_selection import cross_val_score\n",
        " from sklearn.tree import DecisionTreeClassifier\n",
        " scores = cross_val_score(DecisionTreeClassifier(),x,y,cv = 5)\n",
        " Scores\n",
        " array([0.70779221, 0.67532468, 0.68181818, 0.79738562, 0.73202614])\n",
        " scores.mean()\n",
        "\n",
        " from sklearn.ensemble import RandomForestClassifier\n",
        " scores = cross_val_score(RandomForestClassifier(n_estimators = 40),x,y,cv = 5)\n",
        " scores.mean()\n",
        "\n",
        " from sklearn.ensemble import BaggingClassifier\n",
        " bag_model = BaggingClassifier(\n",
        " base_estimator = DecisionTreeClassifier(),\n",
        " n_estimators = 100,\n",
        " max_samples = 0.8,\n",
        " oob_score = True,\n",
        " random_state = 0\n",
        " )\n",
        " bag_model.fit(x_train,y_train)\n",
        " bag_model.oob_score_\n",
        "\n",
        " bag_model.fit(x_train,y_train)\n",
        " bag_model.oob_score_\n",
        "\n",
        " from sklearn import metrics\n",
        " from sklearn.metrics import accuracy_score\n",
        " # Assuming 'bag_model' is your trained model\n",
        " y_pred = bag_model.predict(x_test) # Generate predictions\n",
        " # Nowcalculate the accuracy\n",
        " accuracy = accuracy_score(y_test, y_pred)\n",
        " print(accuracy)\n",
        "\n",
        " bag_model.score(x_test,y_test)\n",
        " bag_model.oob_score_\n",
        "\n",
        " from sklearn import metrics\n",
        " from sklearn.metrics import accuracy_score\n",
        " metrics.accuracy_score(y_test,y_pred)\n",
        "\n",
        " bag_model = BaggingClassifier(\n",
        " base_estimator = DecisionTreeClassifier(),\n",
        " n_estimators = 100,\n",
        " max_samples = 0.8,\n",
        " oob_score = True,\n",
        " random_state = 0\n",
        " )\n",
        " scores = cross_val_score(bag_model,x,y,cv = 5)\n",
        " scores\n",
        "\n",
        " scores.mean()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        " #y_pred = bag_model.predict(x_test)\n",
        " print(classification_report(y_test,y_pred))\n",
        "\n",
        " from matplotlib import pyplot as plt\n",
        " from sklearn.metrics import confusion_matrix\n",
        " import seaborn as sns\n",
        " mat = confusion_matrix(y_test,y_pred)\n",
        " sns.heatmap(mat,square = True,annot = True,fmt = 'd',cbar = False)\n",
        " plt.xlabel('true label')\n",
        " plt.ylabel('predicted label');"
      ],
      "metadata": {
        "id": "2zbIkRbrxNxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "S-NTd87RyQ0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  pandas  as  pd\n",
        "import  numpy  as  np\n",
        "import  matplotlib.pyplot  as  plt\n",
        "from  sklearn.model_selection  import  train_test_split\n",
        "from  sklearn.svm  import  SVC\n",
        "from  sklearn.metrics  import  accuracy_score\n",
        "from  sklearn.preprocessing  import  StandardScaler\n",
        "diabetes_data = pd.read_csv( '/content/diabetes (1).csv' )\n",
        "# Explore the dataset (optional)\n",
        "print (diabetes_data.head())\n",
        "print (diabetes_data.info())\n",
        " #  Assuming  the  last  column  is  the  target  variable  (0  for  non-diabetic,  1\n",
        " for diabetic)\n",
        " X = diabetes_data.iloc[:, :  -1  ].values\n",
        " y = diabetes_data.iloc[:,  -1  ].values\n",
        " # Split the dataset into training and testing sets\n",
        " X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size=  0.2  ,\n",
        " random_state=  42  )\n",
        " # Standardize the features\n",
        " scaler = StandardScaler()\n",
        " X_train = scaler.fit_transform(X_train)\n",
        " X_test = scaler.transform(X_test)\n",
        " # Create an instance of the SVM classifier\n",
        " clf = SVC(kernel=  'linear'  )\n",
        " # Train the SVM classifier\n",
        " clf.fit(X_train, y_train)\n",
        " # Make predictions on the test set\n",
        " y_pred = clf.predict(X_test)\n",
        " # Evaluate the performance of the model\n",
        " accuracy = accuracy_score(y_test, y_pred)\n",
        " print  (  f  'Accuracy:  {accuracy  :.2f  }  '  )\n",
        " # Visualize decision boundary (Modified)\n",
        " def  plot_decision_boundary  (  clf  ,  X  ,  y  ):\n",
        " # Note: This visualization is simplified for 2D.\n",
        " # For higher dimensions, techniques like PCA or  t-SNE are needed.\n",
        " # The following line is changed to use all features  of X_test\n",
        " Z = clf.predict(X)\n",
        " # The rest of the plotting code remains the same,\n",
        " # but it will now use predictions based on all  features.\n",
        " plt.scatter(X[:,  0  ], X[:,  1  ], c=Z, edgecolors=  'k'  ,  marker=  'o'  )\n",
        " plt.title(  'SVM Decision Regions (First Two Features)'  )\n",
        " plt.xlabel(  'Feature 1'  )\n",
        " plt.ylabel(  'Feature 2'  )\n",
        " plt.show()\n",
        " # Plot the decision boundary (Using all features of X_test)\n",
        " plot_decision_boundary(clf, X_test, y_test)"
      ],
      "metadata": {
        "id": "Lq66W3-dyRku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA (Principal Component Analysis)"
      ],
      "metadata": {
        "id": "dR1A0GSQyWzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "winedata = load_wine()\n",
        "X, y = winedata['data'], winedata['target']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X[:,1], X[:,2], c=y)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "Xt = pca.fit_transform(X)\n",
        "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
        "plt.legend(labels=list(winedata['target_names']))\n",
        "plt.show()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "plt.figure(figsize=(8,6))\n",
        "Xt = pipe.fit_transform(X)\n",
        "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
        "plt.legend(labels=list(winedata['target_names']))\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"First two principal components after scaling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KTgNTfsCyZmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Singular Value Decomposition (SVD)"
      ],
      "metadata": {
        "id": "-mF28Sf3yiXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Function to compute SVD and reconstruct the matrix\n",
        "def svd_decomposition(matrix):\n",
        "# Compute SVD\n",
        "U, S, Vt = np.linalg.svd(matrix, full_matrices=False)  # Use reduced SVD\n",
        "# Create the diagonal matrix of singular values\n",
        "Sigma = np.diag(S)  # No need to manually create a zero matrix\n",
        "# Reconstruct the original matrix using U, Sigma, and Vt\n",
        "A_reconstructed = np.dot(U, np.dot(Sigma, Vt))\n",
        "return U, S, Vt, A_reconstructed\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "# Input matrix dimensions\n",
        "rows = int(input(\"Enter the number of rows: \"))\n",
        "cols = int(input(\"Enter the number of columns: \"))\n",
        "# Input matrix elements\n",
        "print(f\"Enter the elements of the {rows}x{cols} matrix row by row:\")\n",
        "matrix = []\n",
        "for i in range(rows):\n",
        "row = list(map(float, input(f\"Enter row {i+1}: \").split()))\n",
        "matrix.append(row)\n",
        "# Convert the list to a NumPy array\n",
        "matrix = np.array(matrix)\n",
        "# Perform SVD\n",
        "U, S, Vt, A_reconstructed = svd_decomposition(matrix)\n",
        "# Output results\n",
        "print(\"\\nInput Matrix:\")\n",
        "print(matrix)\n",
        "print(\"\\nU matrix (Left singular vectors):\")\n",
        "print(U)\n",
        "print(\"\\nSingular values (Diagonal of Σ):\")\n",
        "print(S)\n",
        "print(\"\\nVt matrix (Transpose of Right singular vectors):\")\n",
        "print(Vt)\n",
        "print(\"\\nReconstructed Matrix (Using U, Σ, Vt):\")\n",
        "print(A_reconstructed)"
      ],
      "metadata": {
        "id": "jvIfwYEVynz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}